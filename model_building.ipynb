{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building of model based on this paper:\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1303070121000329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hexuity as hx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/nm0sr8dn1vd806c3239_r7k00000gn/T/ipykernel_33578/1292503179.py:16: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  all_speeches = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "# quick clean\n",
    "\n",
    "speeches, _ = hx.data_expander(price_data=False)\n",
    "\n",
    "import string\n",
    "def strip(dirty):\n",
    "    try:\n",
    "        return ''.join([x for x in dirty if x in string.ascii_letters + '\\'- '])\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "speeches = speeches.applymap(strip)\n",
    "\n",
    "# put speeches in long column for training\n",
    "\n",
    "all_speeches = pd.Series()\n",
    "for col in speeches.columns:\n",
    "    all_speeches = all_speeches.append(speeches.loc[:,col])\n",
    "\n",
    "all_speeches = all_speeches.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer1 = TfidfVectorizer(ngram_range=(1,1), max_features=100, stop_words='english')\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(2,2), max_features=100, stop_words='english')\n",
    "vectorizer3 = TfidfVectorizer(ngram_range=(3,3), max_features=250, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer1.fit(all_speeches)\n",
    "#vectorizer2.fit(all_speeches)\n",
    "vectorizer3.fit(all_speeches)\n",
    "\n",
    "vectorizers = [vectorizer3] #[vectorizer1, vectorizer2, vectorizer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008333333333333333\n",
      "0.016666666666666666\n",
      "0.025\n",
      "0.03333333333333333\n",
      "0.041666666666666664\n",
      "0.05\n",
      "0.058333333333333334\n",
      "0.06666666666666667\n",
      "0.075\n",
      "0.08333333333333333\n",
      "0.09166666666666666\n",
      "0.1\n",
      "0.10833333333333334\n",
      "0.11666666666666667\n",
      "0.125\n",
      "0.13333333333333333\n",
      "0.14166666666666666\n",
      "0.15\n",
      "0.15833333333333333\n",
      "0.16666666666666666\n",
      "0.175\n",
      "0.18333333333333332\n",
      "0.19166666666666668\n",
      "0.2\n",
      "0.20833333333333334\n",
      "0.21666666666666667\n",
      "0.225\n",
      "0.23333333333333334\n",
      "0.24166666666666667\n",
      "0.25\n",
      "0.25833333333333336\n",
      "0.26666666666666666\n",
      "0.275\n",
      "0.2833333333333333\n",
      "0.2916666666666667\n",
      "0.3\n",
      "0.30833333333333335\n",
      "0.31666666666666665\n",
      "0.325\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "tokenised_speeches = pd.DataFrame(index=np.arange(len(speeches)))\n",
    "i = 0\n",
    "limit = 120\n",
    "for vectorizer in vectorizers:\n",
    "    for col in speeches.columns:\n",
    "        temp_tokens = pd.DataFrame(vectorizer.transform(speeches.loc[:,col]).toarray())\n",
    "        tokenised_speeches = pd.concat([tokenised_speeches,temp_tokens], axis=1, ignore_index=True)\n",
    "        i+=1\n",
    "        print(i/limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick model test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenised_speeches\n",
    "\n",
    "X_prices, y = hx.data_expander(price_data=True, speech_data=False, task='regression')\n",
    "X_prices.index=np.arange(len(X_prices))\n",
    "\n",
    "X = pd.concat([X, X_prices], axis=1, ignore_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = hx.data_expander(speech_data=False, task='regression')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3606053997459688"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer = XGBRegressor()\n",
    "classifer.fit(X_train, y_train)\n",
    "preds = classifer.predict(X_test)\n",
    "mean_squared_error(preds, y_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166716391893491"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer = XGBRegressor()\n",
    "classifer.fit(X_train, y_train)\n",
    "preds = classifer.predict(X_test)\n",
    "mean_squared_error(preds, y_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.446849024551102"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer = XGBRegressor()\n",
    "classifer.fit(X_train, y_train)\n",
    "preds = classifer.predict(X_test)\n",
    "mean_squared_error(preds, y_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjalmarheld/miniforge3/envs/ds2/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:36:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.605420403143"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer = XGBClassifier()\n",
    "classifer.fit(X_train, y_train)\n",
    "preds = classifer.predict(X_test)\n",
    "mean_squared_error(preds, y_test, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification / Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "def df_tokenizer(DF):\n",
    "\n",
    "\n",
    "    orig_index = DF.index\n",
    "\n",
    "    # put speeches in long column for training\n",
    "\n",
    "    all_speeches = pd.Series()\n",
    "    for col in DF.columns:\n",
    "        all_speeches = all_speeches.append(DF.loc[:,col])\n",
    "\n",
    "    all_speeches = all_speeches.drop_duplicates()\n",
    "\n",
    "\n",
    "    #vectorizer1 = TfidfVectorizer(ngram_range=(1,1), max_features=100, stop_words='english')\n",
    "    #vectorizer2 = TfidfVectorizer(ngram_range=(2,2), max_features=100, stop_words='english')\n",
    "    vectorizer3 = TfidfVectorizer(ngram_range=(3,3), max_features=250, stop_words='english')\n",
    "\n",
    "    #vectorizer1.fit(all_speeches)\n",
    "    #vectorizer2.fit(all_speeches)\n",
    "    vectorizer3.fit(all_speeches)\n",
    "\n",
    "    vectorizers = [vectorizer3]\n",
    "\n",
    "    tokenised_speeches = pd.DataFrame(index=np.arange(len(DF)))\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for col in DF.columns:\n",
    "            temp_tokens = pd.DataFrame(vectorizer.transform(DF.loc[:,col]).toarray())\n",
    "            tokenised_speeches = pd.concat([tokenised_speeches,temp_tokens], axis=1, ignore_index=True)\n",
    "\n",
    "    tokenised_speeches.index = orig_index\n",
    "\n",
    "    return tokenised_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "tokenized = FunctionTransformer(df_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(DF):\n",
    "    price_columns = DF.select_dtypes(include=np.number).columns.tolist()\n",
    "    speech_columns = DF.select_dtypes(include=object).columns.tolist()\n",
    "\n",
    "    X_price = DF[price_columns]\n",
    "    X_speech = DF[speech_columns]\n",
    "\n",
    "    import string\n",
    "    \n",
    "    def strip(dirty):\n",
    "        try:\n",
    "            return ''.join([x for x in dirty if x in string.ascii_letters + '\\'- '])\n",
    "        except:\n",
    "            return 'the'\n",
    "\n",
    "    X_speech = X_speech.applymap(strip)\n",
    "\n",
    "    X_speech = df_tokenizer(X_speech)\n",
    "\n",
    "    out_df = pd.concat([X_price, X_speech], axis=1)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_pre = FunctionTransformer(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([\n",
    "    ('pre', sk_pre),\n",
    "    ('classifier', XGBRegressor())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = hx.data_expander(task='regression', test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534    -0.812528\n",
       "269     0.015305\n",
       "1160    0.383347\n",
       "197     1.368310\n",
       "97     -0.745036\n",
       "          ...   \n",
       "417    -0.671216\n",
       "1154   -0.673325\n",
       "865    -0.822019\n",
       "455    -1.067732\n",
       "1042    0.778809\n",
       "Name: target_reg, Length: 802, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/nm0sr8dn1vd806c3239_r7k00000gn/T/ipykernel_36674/1573586559.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  all_speeches = pd.Series()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre',\n",
       "                 FunctionTransformer(func=<function pre at 0x2e42fce50>)),\n",
       "                ('classifier',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, enable_categorical=False,\n",
       "                              gamma=0, gpu_id=-1, importance_type=None,\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
       "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/nm0sr8dn1vd806c3239_r7k00000gn/T/ipykernel_36674/1573586559.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  all_speeches = pd.Series()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4250874931485275"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = test_pipe.predict(X_test)\n",
    "mean_squared_error(preds, y_test, squared=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31d8a1ef34cbcebda31b01d96f0668cc5da4b9d1ec201e7411d63c9c214da37f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ds2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
